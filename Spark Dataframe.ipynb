{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93776369",
   "metadata": {},
   "source": [
    "# Spark DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371b5617",
   "metadata": {},
   "source": [
    "Spark dataframes have three basics as follow:\n",
    "- Creation\n",
    "- Manipulation\n",
    "- User Defined Functions\n",
    "\n",
    "Spark dataframes are spark datasets organized into named columns. They are very similart to pandas dataframes.\n",
    "All of the interaction with dataframes is done via sparksessions. To start programming spark with dataframe API, we need to create spark session. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a5962a",
   "metadata": {},
   "source": [
    "# Create SparkSession:\n",
    "SparkSessions are how we interact with Spark Data Frame. To create SparkSession, We need to use a builder pattern as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4dd469e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/10/28 14:50:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .appName(\"Python Spark SQL basic example\") \\\n",
    "        .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f36151",
   "metadata": {},
   "source": [
    "- Dataframes can be created using SparkSession.\n",
    "- DataFrames can be created from:\n",
    "   - a list(of Tuples or values), \n",
    "   - an RDD(Resilient Distributed Dataset), or\n",
    "   - specially-formatted Jason file. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d345a379",
   "metadata": {},
   "source": [
    "# Let's read our json file datasets:\n",
    "- We have 3 large json datasets from yelp academic called user, review, and checkin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22216c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "user = spark.read.json('../../assets/data/yelp_academic/yelp_academic_dataset_user.json.gz')\n",
    "review = spark.read.json('../../assets/data/yelp_academic/yelp_academic_dataset_review.json.gz')\n",
    "checkin = spark.read.json('../../assets/data/yelp_academic/yelp_academic_dataset_checkin.json.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df58ee65",
   "metadata": {},
   "source": [
    "# Data Manipulation\n",
    " In this section, we will manipulat large datasets to get relevant information as mentioned below:\n",
    " - -- CUTE COMPLIMENTS --\n",
    " - -- TOP NEGATIVE REVIEWS --\n",
    " - -- CHECKINS --\n",
    " - -- TOP_50_WORD_REVIEW --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a043eea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since the file is so big, we can grab small part of it to get an idea about columns and...\n",
    "user1=user.sample(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "345ebe4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('average_stars', 'double'),\n",
       " ('compliment_cool', 'bigint'),\n",
       " ('compliment_cute', 'bigint'),\n",
       " ('compliment_funny', 'bigint'),\n",
       " ('compliment_hot', 'bigint'),\n",
       " ('compliment_list', 'bigint'),\n",
       " ('compliment_more', 'bigint'),\n",
       " ('compliment_note', 'bigint'),\n",
       " ('compliment_photos', 'bigint'),\n",
       " ('compliment_plain', 'bigint'),\n",
       " ('compliment_profile', 'bigint'),\n",
       " ('compliment_writer', 'bigint'),\n",
       " ('cool', 'bigint'),\n",
       " ('elite', 'string'),\n",
       " ('fans', 'bigint'),\n",
       " ('friends', 'string'),\n",
       " ('funny', 'bigint'),\n",
       " ('name', 'string'),\n",
       " ('review_count', 'bigint'),\n",
       " ('useful', 'bigint'),\n",
       " ('user_id', 'string'),\n",
       " ('yelping_since', 'string')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user1.columns # name of the columns\n",
    "user1.dtypes # name of the columns with data type in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55bcec8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+---------------+----------------+--------------+---------------+---------------+---------------+-----------------+----------------+------------------+-----------------+----+--------------+----+--------------------+-----+---------+------------+------+--------------------+-------------------+\n",
      "|average_stars|compliment_cool|compliment_cute|compliment_funny|compliment_hot|compliment_list|compliment_more|compliment_note|compliment_photos|compliment_plain|compliment_profile|compliment_writer|cool|         elite|fans|             friends|funny|     name|review_count|useful|             user_id|      yelping_since|\n",
      "+-------------+---------------+---------------+----------------+--------------+---------------+---------------+---------------+-----------------+----------------+------------------+-----------------+----+--------------+----+--------------------+-----+---------+------------+------+--------------------+-------------------+\n",
      "|         4.03|              1|              0|               1|             2|              0|              0|              1|                0|               1|                 0|                2|  25|2015,2016,2017|   5|c78V-rj8NQcQjOI8K...|   17|   Rashmi|          95|    84|l6BmjZMeQD3rDxWUb...|2013-10-08 23:11:33|\n",
      "|          3.5|              0|              0|               0|             0|              0|              0|              0|                0|               1|                 0|                1|   3|              |   2|Dg8_xYNvjVC6KGNRc...|    4|Gabriella|          11|    12|FTWKZZeLb_0ZJRXQg...|2012-11-13 00:23:51|\n",
      "+-------------+---------------+---------------+----------------+--------------+---------------+---------------+---------------+-----------------+----------------+------------------+-----------------+----+--------------+----+--------------------+-----+---------+------------+------+--------------------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user1.show(2) # displayes first 2 entries/rows from dataset \"user1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1c6e35",
   "metadata": {},
   "source": [
    "### -- CUTE COMPLIMENTS --\n",
    "- Count the number of users with less than 200 cute compliments (compliment_cute) and save it in a variable calles: cute_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "476eef17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "195"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter() function similar to pandas, we can filetr dataframe according to some conditions to get intended results\n",
    "# We will use count() function to show the number of cute-compliments which are less than 200\n",
    "cute_count = user.filter(user['compliment_cute']>=200).count()\n",
    "cute_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "daec0195",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1636943"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of users with more than or equal 200 cute compliments (compliment_cute)\n",
    "cute_count2 = user.filter(user['compliment_cute']<200).count()\n",
    "cute_count2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7183f29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|compliment_cute|\n",
      "+---------------+\n",
      "|             29|\n",
      "|             26|\n",
      "|            474|\n",
      "|             65|\n",
      "|            418|\n",
      "|           1224|\n",
      "|            270|\n",
      "|            222|\n",
      "|            293|\n",
      "|            730|\n",
      "|            243|\n",
      "|            278|\n",
      "|             19|\n",
      "|            296|\n",
      "|             54|\n",
      "|              0|\n",
      "|            287|\n",
      "|            277|\n",
      "|            348|\n",
      "|            113|\n",
      "+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# distinct() will help us find the distinct values of specific column chosen by select() and show() will return 20 of them \n",
    "\n",
    "user.select('compliment_cute').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04478da",
   "metadata": {},
   "source": [
    "### -- TOP NEGATIVE REVIEWS --"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08d2403",
   "metadata": {},
   "source": [
    "Let's find the top 10 negative reviews:\n",
    "- Create a spark dataframe named: top_10_negative_reviews\n",
    "- For our purpose, negative reviwes are the ones with 1 or 2 stars (according to distinct we only have 1,2,3,4 & 5 as stars)\n",
    "- Created dataframe should be ordered by funny and contain only 10 rows\n",
    "- top_10_negative_reviews dataframe should have the folloiwng columns: user_id, review_id, funny, stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "205c8db1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('business_id', 'string'),\n",
       " ('cool', 'bigint'),\n",
       " ('date', 'string'),\n",
       " ('funny', 'bigint'),\n",
       " ('review_id', 'string'),\n",
       " ('stars', 'double'),\n",
       " ('text', 'string'),\n",
       " ('useful', 'bigint'),\n",
       " ('user_id', 'string')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review.columns\n",
    "review.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0008c1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|stars|\n",
      "+-----+\n",
      "|  1.0|\n",
      "|  4.0|\n",
      "|  3.0|\n",
      "|  2.0|\n",
      "|  5.0|\n",
      "+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "review.select('stars').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a09fac4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+-------------------+-----+--------------------+-----+--------------------+------+--------------------+\n",
      "|         business_id|cool|               date|funny|           review_id|stars|                text|useful|             user_id|\n",
      "+--------------------+----+-------------------+-----+--------------------+-----+--------------------+------+--------------------+\n",
      "|ujmEBvifdJM6h6RLv...|   0|2013-05-07 04:34:36|    1|Q1sbwvVQXV2734tPg...|  1.0|Total bill for th...|     6|hG7b0MtEbXx5QzbzE...|\n",
      "|NZnhc2sEQy3RmzKTZ...|   0|2017-01-14 21:30:33|    0|GJXCdrto3ASJOqKeV...|  5.0|I *adore* Travis ...|     0|yXQM5uF2jS6es16SJ...|\n",
      "+--------------------+----+-------------------+-----+--------------------+-----+--------------------+------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6fcbaea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[user_id: string, review_id: string, funny: bigint, stars: double]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top 10 negative reviews sorted by another column \"funny\"\n",
    "top_10_negative_reviews=review.select(['user_id','review_id','funny','stars']).filter(review['stars']<3).sort('funny', ascending = False)\n",
    "top_10_negative_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "634deee8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+-----+\n",
      "|             user_id|           review_id|funny|stars|\n",
      "+--------------------+--------------------+-----+-----+\n",
      "|qiTy11I-yp6foxIgh...|A8mLBytNM2zmjHgSp...|  628|  1.0|\n",
      "|BbSWL3PjQBzY7uDWa...|ZmEtySx0W_RSv07aY...|  332|  1.0|\n",
      "|H5dkC0OmZkRDTFjN4...|xophRrPX3yig5psGd...|  287|  1.0|\n",
      "|AsUDg2wZZqkgZzl0k...|4ZN5ZWVoGd8er9giA...|  277|  2.0|\n",
      "|p_2daiuEk774FFHsK...|c6lCQwW9oeq903z7Y...|  267|  1.0|\n",
      "|XmIsBXpdBevTYCe8g...|69suGHxR3PVxicAgm...|  266|  1.0|\n",
      "|TG3GcRx20tAxXxpqn...|EYbuFrEnVkVdavuRm...|  241|  1.0|\n",
      "|-nlHAaKCQF5I0Gbbw...|NVeCBLhxOBQbSGLIe...|  237|  1.0|\n",
      "|36kREh8Oib7RdVyaT...|PZVlLaH6SJoSSigLL...|  217|  1.0|\n",
      "|Tyb_FnUv3L0LqsYyL...|i9LyzLWSozIe6fIiL...|  208|  2.0|\n",
      "+--------------------+--------------------+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "top_10_negative_reviews = top_10_negative_reviews.limit(10)\n",
    "top_10_negative_reviews.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1267413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(top_10_negative_reviews) # top_10_negative_reviews is a spark dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49266e3",
   "metadata": {},
   "source": [
    "## -- CHECKINS --\n",
    "Let's determine what hours of the day, the least number of checkins occur:\n",
    "- Create a spark dataframe called checkin_hour_least\n",
    "- Order dataframe by count, with only 20 top rows\n",
    "- The following columns should be selected in the dataframe:\n",
    "  -  \"hour\": The hour of the day as an integer and the hour after midnight being as 0\n",
    "  -  \"count\": the number of checkin occured in that hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47e0c3a2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(business_id='--1UhMGODdWsrMastO9DZw', date='2016-04-26 19:49:16, 2016-08-30 18:36:57, 2016-10-15 02:45:18, 2016-11-18 01:54:50, 2017-04-20 18:39:06, 2017-05-03 17:58:02')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkin.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b360788c",
   "metadata": {},
   "source": [
    "- Our date column in checkin dataset is made of multiple date times. Therefore, we need a User Defined Function to split them before parsing.\n",
    "- UDFs (User Defined Functions) are a way of wrapping python defined functions that we want to apply to rows. \n",
    "- The reason we need to wrap them is because of spark being strongly typed language. Spark needs a lot of help figuring out what type, numbers or any objects are. It is because it comes from Scala or Java where we have strongly typed variables. In spark we have to be very strict about what type a number is. When we are going to use plain python functions, we need to specify the output type. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af02604",
   "metadata": {},
   "source": [
    "## Import Important Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3884bf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.sql.functions import explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb217623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's defind UDF to apply a split() function from python into a column we are interested in. \n",
    "# we will get an array of string types which is mentioned in the line below\n",
    "\n",
    "split_date_udf = udf(lambda x: x.split(\",\"), ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b8543c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):                                  (0 + 1) / 1]\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 187, in manager\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 730, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(date='2016-04-26 19:49:16, 2016-08-30 18:36:57, 2016-10-15 02:45:18, 2016-11-18 01:54:50, 2017-04-20 18:39:06, 2017-05-03 17:58:02', Dates=['2016-04-26 19:49:16', ' 2016-08-30 18:36:57', ' 2016-10-15 02:45:18', ' 2016-11-18 01:54:50', ' 2017-04-20 18:39:06', ' 2017-05-03 17:58:02'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we can apply udf to our date column in checkin dataset & get only first row for our information\n",
    "\n",
    "checkin.select('date', split_date_udf('date').alias('Dates')).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e251dc43",
   "metadata": {},
   "source": [
    "- Now, we will explode the values in each row of 'Dates' column since we have multiple dates in our 'date' column. but save them in separate column called 'cjheckin_hours'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "129a49f3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                date|               Dates|       checkin_hours|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|2016-04-26 19:49:...|[2016-04-26 19:49...| 2016-04-26 19:49:16|\n",
      "|2016-04-26 19:49:...|[2016-04-26 19:49...| 2016-08-30 18:36:57|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataframe = checkin.select('date', split_date_udf('date').alias('Dates')).withColumn('checkin_hours', explode('Dates'))\n",
    "dataframe.show(2)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3716bd29",
   "metadata": {},
   "source": [
    "- Our new 'checkin_hours' column is string type. we need to convert it to DateType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94132430",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('date', 'string'), ('Dates', 'array<string>'), ('checkin_hours', 'string')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "967e1e7f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('date', 'string'),\n",
       " ('Dates', 'array<string>'),\n",
       " ('checkin_hours', 'string'),\n",
       " ('timestamp_checkin_hour', 'timestamp')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's change the checkin_hours data type from string to DateType(we need the followig libraries)\n",
    "#from pyspark.sql.functions import hour\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "\n",
    "dataframe.withColumn('timestamp_checkin_hour', to_timestamp('checkin_hours')).dtypes\n",
    "# Our 'timestamp_checkin_hour' column has a timestamp type values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4c76219b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+----------------------+\n",
      "|                date|               Dates|       checkin_hours|timestamp_checkin_hour|\n",
      "+--------------------+--------------------+--------------------+----------------------+\n",
      "|2016-04-26 19:49:...|[2016-04-26 19:49...| 2016-04-26 19:49:16|   2016-04-26 19:49:16|\n",
      "|2016-04-26 19:49:...|[2016-04-26 19:49...| 2016-08-30 18:36:57|   2016-08-30 18:36:57|\n",
      "|2016-04-26 19:49:...|[2016-04-26 19:49...| 2016-10-15 02:45:18|   2016-10-15 02:45:18|\n",
      "+--------------------+--------------------+--------------------+----------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataframe = dataframe.withColumn('timestamp_checkin_hour', to_timestamp('checkin_hours'))\n",
    "dataframe.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1b4bf779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[timestamp_checkin_hour: timestamp, hour: int]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, let's extract the integer hours from our 'timestamp_checkin_hour' column using hour(df.timestamp) and rename it as 'hour'\n",
    "#we need udf\n",
    "hour_udf = udf(lambda x: x.hour, IntegerType())\n",
    "\n",
    "#let's apply udf to our dataframe column:\n",
    "dataframe = dataframe.select('timestamp_checkin_hour', hour_udf('timestamp_checkin_hour').alias('hour'))\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "78c2bf66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 22:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+----+\n",
      "|timestamp_checkin_hour|hour|\n",
      "+----------------------+----+\n",
      "|   2016-04-26 19:49:16|  19|\n",
      "|   2016-08-30 18:36:57|  18|\n",
      "|   2016-10-15 02:45:18|   2|\n",
      "|   2016-11-18 01:54:50|   1|\n",
      "|   2017-04-20 18:39:06|  18|\n",
      "|   2017-05-03 17:58:02|  17|\n",
      "|   2011-06-04 18:22:23|  18|\n",
      "|   2011-07-23 23:51:33|  23|\n",
      "|   2012-04-15 01:07:50|   1|\n",
      "|   2012-05-06 23:08:42|  23|\n",
      "|   2012-06-08 22:43:12|  22|\n",
      "|   2012-08-06 23:20:52|  23|\n",
      "|   2012-08-19 18:30:44|  18|\n",
      "|   2013-01-27 23:49:51|  23|\n",
      "|   2013-03-01 01:22:29|   1|\n",
      "|   2013-03-23 21:53:47|  21|\n",
      "|   2013-03-24 01:11:51|   1|\n",
      "|   2013-05-20 00:12:25|   0|\n",
      "|   2013-06-29 22:50:57|  22|\n",
      "|   2013-07-01 15:58:04|  15|\n",
      "+----------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dataframe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4f849ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "|hour|  count|\n",
      "+----+-------+\n",
      "|  12| 178910|\n",
      "|  22|1257437|\n",
      "|   1|1561788|\n",
      "|  13| 270145|\n",
      "|  16| 852076|\n",
      "+----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Let's count the number of each checkin hour during the day, first by groupBy on 'hour' column and apply count():\n",
    "\n",
    "dataframe.groupBy('hour').count().show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "efe7f2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First count the grouped hours and then sorted on counted values:\n",
    "\n",
    "conted_hours = dataframe.groupBy('hour').count().alias('counted_hours')\n",
    "sorted_hours = conted_hours.sort('count', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e0bb09bb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 26:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "|hour|  count|\n",
      "+----+-------+\n",
      "|  10|  88486|\n",
      "|   9| 100568|\n",
      "|  11| 111769|\n",
      "|   8| 151065|\n",
      "|  12| 178910|\n",
      "|   7| 231417|\n",
      "|  13| 270145|\n",
      "|   6| 321764|\n",
      "|  14| 418340|\n",
      "|   5| 485129|\n",
      "|  15| 617830|\n",
      "|   4| 747453|\n",
      "|  16| 852076|\n",
      "|  17|1006102|\n",
      "|   3|1078939|\n",
      "|  21|1238808|\n",
      "|  22|1257437|\n",
      "|  18|1272108|\n",
      "|  23|1344117|\n",
      "|  20|1350195|\n",
      "+----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sorted_hours.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ed83bc",
   "metadata": {},
   "source": [
    "- The least number of checkins occur in the 10, 9, 11, 8, 12 and 7 hour (before noon)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "37214611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sorted_hours)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce2d096",
   "metadata": {},
   "source": [
    "- sorted_hours is a spark dataframe containing checkin hours and counts of checking hours from our checkin dataset\n",
    "- The top 20 least checkin hours count is summarized above in the show() display function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99307807",
   "metadata": {},
   "source": [
    "## -- COMMON WORDS IN USEFUL REVIEWS --\n",
    "\n",
    "We want to find the 50 most common words from *useful* reviews and their counts.\n",
    "\n",
    "- A \"useful review\" has 10 or more \"useful\" ratings.\n",
    "- We will focus on he 'text' column.\n",
    "- Convert the 'text' to lower case.\n",
    "- Use the provided `splitter()` function in a UDF to split the text into individual words.\n",
    "- Exclude the words that are in the provided `STOP_WORDS` set.\n",
    "- Final DataFrame should have the following columns:\n",
    "    - `word`\n",
    "    - `count`\n",
    "- DataFrame should be sorted by `count` in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "57d6adbf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+-------------------+-----+--------------------+-----+--------------------+------+--------------------+\n",
      "|         business_id|cool|               date|funny|           review_id|stars|                text|useful|             user_id|\n",
      "+--------------------+----+-------------------+-----+--------------------+-----+--------------------+------+--------------------+\n",
      "|sMzNLdhJZGzYirIWt...|   0|2015-06-21 00:59:14|    0|RLbWoIri29BcQ8yjz...|  5.0|This place epitom...|     0|_o740mSNRhMNYuPjS...|\n",
      "|dZVMp70AuSa4dQPvx...|   1|2011-09-25 18:10:04|    0|IRwIFWgjJiMSBBuce...|  3.0|In need of a burg...|     2|PFNZVn73upq3oZDG2...|\n",
      "|yNPh5SO-7wr8HPpVC...|   0|2017-04-30 05:32:05|    0|JYdhCDyR6lYfN2qnS...|  5.0|First off food is...|     0|6kEFHccntnYMF_7cd...|\n",
      "+--------------------+----+-------------------+-----+--------------------+-----+--------------------+------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's see a small part of the review:\n",
    "review.sample(0.01).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2fa6e6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitter() function will split the text into words:\n",
    "import re\n",
    "def splitter(text):\n",
    "    WORD_RE = re.compile(r\"[\\w']+\")\n",
    "    return WORD_RE.findall(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "339b3970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our STOWORDS\n",
    "STOP_WORDS = {\n",
    "    \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"aint\", \"all\", \"also\", \"although\", \"am\", \"an\", \"and\", \"any\",\n",
    "    \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"can\",\n",
    "    \"check\", \"checked\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"don\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\",\n",
    "    \"further\", \"get\", \"go\", \"got\", \"had\", \"has\", \"have\", \"having\", \"he\", \"her\", \"here\", \"hers\", \"herself\", \"him\",\n",
    "    \"himself\", \"his\", \"how\", \"however\", \"i\", \"i'd\", \"if\", \"i'm\", \"in\", \"into\", \"is\", \"it\", \"its\", \"it's\", \"itself\",\n",
    "    \"i've\", \"just\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"no\", \"nor\", \"not\", \"now\", \"of\", \"off\", \"on\", \"once\", \"one\",\n",
    "    \"online\", \"only\", \"or\", \"other\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"paid\", \"place\", \"s\", \"said\", \n",
    "    \"same\", \"service\", \"she\", \"should\", \"so\", \"some\", \"such\", \"t\", \"than\", \"that\", \"the\", \"their\", \"theirs\", \"them\",\n",
    "    \"themselves\", \"then\", \"there\", \"these\", \"they\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\",\n",
    "    \"us\", \"very\", \"was\", \"we\", \"went\", \"were\", \"we've\", \"what\", \"when\", \"where\", \"which\", \"while\", \"who\", \"whom\",\n",
    "    \"why\", \"will\", \"with\", \"would\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "70e0e797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+-------------------+-----+--------------------+-----+--------------------+------+--------------------+\n",
      "|         business_id|cool|               date|funny|           review_id|stars|                text|useful|             user_id|\n",
      "+--------------------+----+-------------------+-----+--------------------+-----+--------------------+------+--------------------+\n",
      "|1HD5iUUfVJDbfEBIn...|  11|2017-03-15 02:02:13|    8|qdQIIf6xuyubxEG05...|  5.0|Yes... the Boba T...|    11|XPZVfP7DQCSL3Nb9t...|\n",
      "|PycR_Mr5jA9jB4Xg3...|   1|2014-01-17 02:15:25|    1|h7Rmb3EiXjajVfGYN...|  1.0|They charged me t...|    14|oAOE4UAC5ZbAjEGBE...|\n",
      "|dLxT3-EwXkrI9AXoW...|   0|2016-01-30 01:29:21|    0|LqShY--VVp_0lkgau...|  1.0|PLEASE. Read this...|    10|scitRtsLa4QP9S1LZ...|\n",
      "|tqN30ZVHlmxbI-uQ4...|   9|2016-04-11 17:11:35|    6|jQxm5RANNaqF6AV7l...|  5.0|I so disagree wit...|    11|wzuxPP-d18Mu_IooK...|\n",
      "+--------------------+----+-------------------+-----+--------------------+-----+--------------------+------+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First, we will filter the dataset on 'useful' column for the values of 10 or more\n",
    "df = review\n",
    "df.filter(df['useful']>=10).show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7c60e50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(df['useful']>=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "018519ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We should make the words in the 'text' column lowercase and check if they are in the StopWords\n",
    "# To do that, first import lower function and apply it to the text\n",
    "from pyspark.sql.functions import lower\n",
    "\n",
    "df = df.withColumn('text', lower(df['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "12693042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+-------------------+-----+--------------------+-----+--------------------+------+--------------------+\n",
      "|         business_id|cool|               date|funny|           review_id|stars|                text|useful|             user_id|\n",
      "+--------------------+----+-------------------+-----+--------------------+-----+--------------------+------+--------------------+\n",
      "|1HD5iUUfVJDbfEBIn...|  11|2017-03-15 02:02:13|    8|qdQIIf6xuyubxEG05...|  5.0|yes... the boba t...|    11|XPZVfP7DQCSL3Nb9t...|\n",
      "|PycR_Mr5jA9jB4Xg3...|   1|2014-01-17 02:15:25|    1|h7Rmb3EiXjajVfGYN...|  1.0|they charged me t...|    14|oAOE4UAC5ZbAjEGBE...|\n",
      "+--------------------+----+-------------------+-----+--------------------+-----+--------------------+------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('text', lower(df['text'])).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2afeacbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "udf_splitter = udf(lambda x: splitter(x), ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6a327e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we will apply udf on out 'text' column of data with useful>=10\n",
    "\n",
    "df = df.select('user_id', udf_splitter('text').alias('split_words'))\n",
    "# We have a list of words in each row of 'text' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d30ab8e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|             user_id|         split_words|\n",
      "+--------------------+--------------------+\n",
      "|XPZVfP7DQCSL3Nb9t...|[yes, the, boba, ...|\n",
      "|oAOE4UAC5ZbAjEGBE...|[they, charged, m...|\n",
      "|scitRtsLa4QP9S1LZ...|[please, read, th...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 187, in manager\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 730, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n"
     ]
    }
   ],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "849e6139",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('user_id', 'string'), ('split_words', 'array<string>')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461b84bd",
   "metadata": {},
   "source": [
    "## StopWordsRemover\n",
    "In Spark, the StopWordsRemover is a feature in the pyspark.ml.feature module that is used to remove common stop words from a text. Stop words are words that are often filtered out in natural language processing tasks, such as \"the,\" \"is,\" \"in,\" etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4049b3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d76609d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS=list(STOP_WORDS)\n",
    "remover = StopWordsRemover(stopWords=STOP_WORDS, inputCol=\"split_words\", outputCol=\"filtered_words\")\n",
    "filtered_df = remover.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ddca652a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|             user_id|         split_words|      filtered_words|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|XPZVfP7DQCSL3Nb9t...|[yes, the, boba, ...|[yes, boba, tea, ...|\n",
      "|oAOE4UAC5ZbAjEGBE...|[they, charged, m...|[charged, twice, ...|\n",
      "|scitRtsLa4QP9S1LZ...|[please, read, th...|[please, read, bu...|\n",
      "|wzuxPP-d18Mu_IooK...|[i, so, disagree,...|[disagree, couple...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 187, in manager\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 730, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n"
     ]
    }
   ],
   "source": [
    "filtered_df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a08af70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will drop the 'split_words' column:\n",
    "\n",
    "filtered_df=filtered_df.drop('split_words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ea15995a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|             user_id|      filtered_words|\n",
      "+--------------------+--------------------+\n",
      "|XPZVfP7DQCSL3Nb9t...|[yes, boba, tea, ...|\n",
      "|oAOE4UAC5ZbAjEGBE...|[charged, twice, ...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 187, in manager\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 730, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n"
     ]
    }
   ],
   "source": [
    "filtered_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9ba3e18c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('user_id', 'string'), ('filtered_words', 'array<string>')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d4e01585",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now we need to explode the 'filtered_words' column to distribute each word into a different row in the same 'filtered_words' column:\n",
    "\n",
    "filtered_df = filtered_df.withColumn('filtered_words', explode('filtered_words'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8bf8159c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+\n",
      "|             user_id|filtered_words|\n",
      "+--------------------+--------------+\n",
      "|XPZVfP7DQCSL3Nb9t...|           yes|\n",
      "|XPZVfP7DQCSL3Nb9t...|          boba|\n",
      "|XPZVfP7DQCSL3Nb9t...|           tea|\n",
      "+--------------------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2017b499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's sorth the column and count the words frewquency:\n",
    "filtered_df = filtered_df.groupBy('filtered_words').count().sort('count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7c08f90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 36:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+\n",
      "|filtered_words| count|\n",
      "+--------------+------+\n",
      "|          like|101251|\n",
      "|          time| 86124|\n",
      "|          good| 83486|\n",
      "|          back| 71308|\n",
      "|          food| 65281|\n",
      "|          even| 58499|\n",
      "|        really| 57687|\n",
      "|         don't| 56146|\n",
      "|         great| 55402|\n",
      "|          well| 48297|\n",
      "|        didn't| 45751|\n",
      "|         first| 43738|\n",
      "|        people| 42768|\n",
      "|          know| 40954|\n",
      "|         never| 40741|\n",
      "|             2| 39573|\n",
      "|          told| 39350|\n",
      "|           day| 38164|\n",
      "|          came| 38098|\n",
      "|          much| 37227|\n",
      "+--------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "filtered_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517adfe1",
   "metadata": {},
   "source": [
    "## Write Top_50_word_review Function\n",
    "\n",
    "- Now, we will write a function which takes a DataFrame as a parameter and number n as the number of rows we are interested from it, returns a Spark DataFrame with 50 most common words in 'text' column of useful reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9227f5b8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c7689bc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = review.select(review['useful'])\n",
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "28e78a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Top_50_word_review(df):\n",
    "    \n",
    "    from pyspark.sql.functions import udf\n",
    "    from pyspark.sql.types import IntegerType\n",
    "    from pyspark.sql.types import ArrayType, StringType\n",
    "    from pyspark.sql.functions import explode\n",
    "    from pyspark.sql.functions import lower\n",
    "    from pyspark.ml.feature import StopWordsRemover\n",
    "    import pyspark.sql.functions as F\n",
    "    import re\n",
    "    \n",
    "    df = df.filter(df['useful'] >= 10)\n",
    "    df = df.withColumn('text', lower(df['text']))\n",
    "    \n",
    "    def splitter(text):\n",
    "        WORD_RE = re.compile(r\"[\\w']+\")\n",
    "        return WORD_RE.findall(text)\n",
    "    \n",
    "    split_udf = udf(lambda x: splitter(x), ArrayType(StringType()))\n",
    "    df = df.select('user_id', split_udf('text').alias('split_words'))\n",
    "    \n",
    "    STOP_WORDS = {\n",
    "    \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"aint\", \"all\", \"also\", \"although\", \"am\", \"an\", \"and\", \"any\",\n",
    "    \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"can\",\n",
    "    \"check\", \"checked\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"don\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\",\n",
    "    \"further\", \"get\", \"go\", \"got\", \"had\", \"has\", \"have\", \"having\", \"he\", \"her\", \"here\", \"hers\", \"herself\", \"him\",\n",
    "    \"himself\", \"his\", \"how\", \"however\", \"i\", \"i'd\", \"if\", \"i'm\", \"in\", \"into\", \"is\", \"it\", \"its\", \"it's\", \"itself\",\n",
    "    \"i've\", \"just\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"no\", \"nor\", \"not\", \"now\", \"of\", \"off\", \"on\", \"once\", \"one\",\n",
    "    \"online\", \"only\", \"or\", \"other\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"paid\", \"place\", \"s\", \"said\", \n",
    "    \"same\", \"service\", \"she\", \"should\", \"so\", \"some\", \"such\", \"t\", \"than\", \"that\", \"the\", \"their\", \"theirs\", \"them\",\n",
    "    \"themselves\", \"then\", \"there\", \"these\", \"they\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\",\n",
    "    \"us\", \"very\", \"was\", \"we\", \"went\", \"were\", \"we've\", \"what\", \"when\", \"where\", \"which\", \"while\", \"who\", \"whom\",\n",
    "    \"why\", \"will\", \"with\", \"would\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\"}\n",
    "    \n",
    "    STOP_WORDS=list(STOP_WORDS)\n",
    "    remover = StopWordsRemover(stopWords=STOP_WORDS, inputCol=\"split_words\", outputCol=\"filtered_words\")\n",
    "    filtered_df = remover.transform(df)\n",
    "    \n",
    "    filtered_df = filtered_df.drop('split_words') \n",
    "    \n",
    "    exploded_df = filtered_df.withColumn('filtered_words', explode('filtered_words'))\n",
    "    \n",
    "    sorted_df = exploded_df.groupBy('filtered_words').count().sort('count', ascending=False)\n",
    "    most_common = sorted_df.limit(50)\n",
    "    \n",
    "    return most_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5fe07da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 47:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+\n",
      "|filtered_words| count|\n",
      "+--------------+------+\n",
      "|          like|101251|\n",
      "|          time| 86124|\n",
      "|          good| 83486|\n",
      "|          back| 71308|\n",
      "|          food| 65281|\n",
      "|          even| 58499|\n",
      "|        really| 57687|\n",
      "|         don't| 56146|\n",
      "|         great| 55402|\n",
      "|          well| 48297|\n",
      "|        didn't| 45751|\n",
      "|         first| 43738|\n",
      "|        people| 42768|\n",
      "|          know| 40954|\n",
      "|         never| 40741|\n",
      "|             2| 39573|\n",
      "|          told| 39350|\n",
      "|           day| 38164|\n",
      "|          came| 38098|\n",
      "|          much| 37227|\n",
      "+--------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "Top_50_word_review(review).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe3ffe0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
